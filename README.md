# LeNet5网络实现

南开大学 机器学习（谢晋老师） EX2（大作业）2023年秋

实验要求：手搓LeNet5神经网络

## 网络结构

`LeNet5`网络共有七层：

1. 卷积层，使用6个大小为$5\times 5$的卷积核。
   + 输入：$1\times32\times32$（分别代表通道数、高度、宽度。下同）
   + 输出：$6\times28\times28$
2. 池化层，使用1个大小为$2\times 2$的卷积核，步长为2。
   + 输入：$6\times28\times28$
   + 输出：$6\times14\times14$
3. 卷积层，使用16个大小为$5\times 5$的卷积核。
   + 输入：$6\times\times14\times14$
   + 输出：$16\times10\times10$
4. 池化层，使用1个大小为$2\times 2$的卷积核， 步长为2。
   + 输入：$16\times10\times10$
   + 输出：$16\times5\times5$
5. 卷积层，使用120个大小为$5\times 5$的卷积核。
   + 输入：$16\times5\times5$
   + 输出：$120\times1\times1$
6. 全连接层。
   + 输入：$120$
   + 输出：$84$
7. 输出层，使用径向基函数。
   + 输入：$84$
   + 输出：$10$

除了输出层，每层之后都要加上`sigmoid`激活函数。

在此基础上，我们在最后一层之后加上了`softmax`和交叉熵损失函数，用于优化输出结果。

其实吧，我感觉输出层应该可以直接换成`softmax`，但是为了尽可能复现原版的网络结构，在此保留径基函数的实现。
