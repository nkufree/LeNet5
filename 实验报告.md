# LeNet5实验报告

卷积计算过程：

循环优化：使用stride将矩阵划分为与kernel计算的每一块，使用tensordot计算featuremap

使用PS绘制十个数字的图像，大小为3点，恰好每个数字大小为12×7

## 实验原理

`LeNet5`网络共有七层：

1. 卷积层，使用6个大小为$5\times 5$的卷积核。
2. 池化层，使用1个大小为$2\times 2$的卷积核。
3. 卷积层，使用16个大小为$5\times 5$的卷积核。
4. 池化层，使用1个大小为$2\times 2$的卷积核。
5. 卷积层，使用120个大小为$5\times 5$的卷积核。
6. 全连接层。
7. 输出层，使用径向基函数。

除了输出层，每层之后都要加上`sigmoid`激活函数。

在此基础上，我们在最后一层之后加上了`softmax`和交叉熵损失函数，用于优化输出结果。

## 卷积层

### 基本思路

使用`Conv`类表示卷积层，卷积层的初始化需要提供如下参数：

1. `input_shape`：输入数据的维度，有三维，分别代表通道数、高度和宽度。
2. `kernel_size`：卷积核大小，在这里认为卷积核的长宽相等。
3. `kernel_num`：卷积核个数，决定输出的通道数

之后需要生成随机权重的卷积核和偏置项。

卷积核有四维，分别代表：

1. 卷积核个数
2. 通道数
3. 卷积核高度
4. 卷积核宽度

偏置项有一维，对应卷积核的个数。

我们将卷积核的权重和偏置项均设置为$[-1,1)$之间的随机数。

### 前向传播

卷积层的前向传播的实现思路为：先将矩阵按照卷积核的大小生成与输出对应的部分，之后再与卷积核进行对应项相乘再相加，得到输出的一项。

举个例子。

> 假设输入的维度为`(1,10,10)`（分别对应通道数、输入图像的高度、输入图像的宽度），每一个卷积核大小为`(1,5,5)`（分别对应通道数、卷积核高度、卷积核宽度），我们使用自定义的`split_by_stride`函数划分输入，得到的划分后的矩阵维度为`(1,6,6,5,5)`（分别对应通道数、输出图像高度、输出图像宽度、卷积核高度、卷积核宽度）。

为了反向传播，需要记录卷积层的输入。在卷面积计算完成后，需要进行`sigmoid`激活函数。

`python`代码实现如下：

```python
def forward(self, input):
    # 对矩阵进行分片
    self.input:np.ndarray = input
    self.input_shape = input.shape
    C, H, W = self.input_shape
    _, row, col = self.output_shape
    split_input = self.split_by_stride(input,(C, row, col, self.kernel_size, self.kernel_size))
    # 使用爱因斯坦求和约定计算卷积
    self.output = np.einsum('ijklm,pilm->pjk', split_input, self.filters)\
        + self.bias.repeat(row * col).reshape(self.output_shape)
    # 计算sigmoid
    self.output = 1 / (1 + np.exp(-self.output))
    self.output_shape = self.output.shape
    return self.output
```

### 反向传播

首先进行`sigmoid`函数的反向传播。下面是用的输入均为传播之后的结果。

#### 对输入求梯度

为了更新前一层的权重，需要更新上一层对输入的梯度，求出对输入的梯度。计算步骤如下：

1. 对上一层输入的梯度矩阵做padding。
2. 将卷积核旋转180°。
3. 使用旋转后的卷积核对padding后的矩阵做卷积运算，得到对输入的梯度。

其中padding的计算方式如下：

> 假设图像的高度为$h$，宽度为$w$，卷积核大小为$s$，步长为1，则
> 1. 上方需要插入空白行数为$s-1$。
> 2. 下方需要插入的空白行数为$s-1+(h-s)%1$。
> 3. 左边需要插入的空白列数为$s-1$。
> 2. 右边需要插入的空白行数为$s-1+(w-s)%1$。

计算卷积的`python`代码如下：

```python
split_padded_input = self.split_by_stride(padded_input, (C, row, col, self.kernel_size, self.kernel_size))
input_grade = np.einsum('ijklm,iplm->pjk', split_padded_input, np.rot90(self.filters, 2, (2,3)))
```

#### 对权重求梯度

为了更新权重，需要计算对权重的梯度。计算步骤如下：

1. 以上一层的输入的梯度矩阵作为卷积核，按照该维度划分卷积层的输入矩阵。
2. 对卷积核和卷积层的输入矩阵做类似卷积的操作。

这里使用了类似卷积的操作，主要原因是输出的通道数和输入的通道数不一致，导致无法做常规的卷积。在更新权重的计算中需要同时保留输出通道数对应的维度和输入通道数对应的维度。

使用爱因斯坦求和约定使得这一个过程的表述变得简单。

计算这两个步骤的`python`代码如下：

```python
split_net_input = self.split_by_stride(self.input, (in_c, self.kernel_size, self.kernel_size, H, W))
weight_grade: np.ndarray = np.einsum('ijklm,plm->pijk', split_net_input, input)
```

#### 对偏置项求梯度

为了更新偏置项，需要对偏置项求梯度。

偏置项梯度的计算非常简单，只需要将不同通道的计算结果分别累加即可。

实现的`python`代码如下：

```python
bias_grade = np.einsum('cij->c', input)
```

## 池化层

### 基本思路

使用`Subsampling`类表示池化层。在初始化池化层时，需要提供以下参数：

1. `input_shape`：输入数据的维度，有三维，分别代表通道数、高度和宽度。
2. `kernel_size`：卷积核大小，在这里认为卷积核的长宽相等。
3. `stride`：卷积核的步长，在本实验中应与卷积核的边长相等。

之后需要生成随机权重的卷积核和偏置项。

卷积核有一维，对应通道数。

偏置项有一维，对应通道数。

我们将卷积核的权重和偏置项均设置为$【-1,1)$之间的随机数。


### 前向传播

在`LeNet5`模型中，池化层的思路为：将卷积核对应的数据求和后乘以权重，再加上偏置项。

和卷积层类似，池化层需要保留这次的输入用于反向传播，计算完成后也需要进行`sigmoid`激活函数。

`python`代码实现如下：

```python
def forward(self, input):
    # 对矩阵进行分片
    self.input = input
    self.input_shape = input.shape
    self.split_input = self.split_by_stride(input, (input.shape[0], self.row, self.col, self.kernel_size, self.kernel_size))
    # 使用爱因斯坦求和约定计算下采样
    C, row, col = self.output_shape
    self.output = np.einsum('ijklm,i->ijk', self.split_input, self.filters)\
        + self.bias.repeat(row * col).reshape(self.output_shape)
    self.output = 1 / (1 + np.exp(-self.output))
    self.output_shape = self.output.shape
    return self.output
```

### 反向传播

首先进行`sigmoid`函数的反向传播。下面是用的输入均为传播之后的结果。

#### 对输入的梯度

为了为上一层提供对输出的偏导，需要计算对输入的梯度。

根据这种方式的前向传播，可以推出对于构成一个输出项的输入来说，它们的梯度相等。所以我们可以计算完成每个输出对应的梯度后，将其重复若干次即可。

`python`代码实现如下：

```python
C, H, W = self.output_shape
in_c, in_h, in_w = self.input_shape
input_grade:np.ndarray = np.einsum('cij,c->cij', input, self.filters)
input_grade = input_grade.repeat(self.kernel_size, axis=1).repeat(self.kernel_size, axis=2)
```

#### 对权重的梯度

对权重的梯度的计算首先要将输入划分为与卷积核对应的部分，然后求和并与输出的对应项相乘，每各通道的结果相加即为对权重的梯度。

由于在前向传播的时候已经计算过输入的划分，所以在前向传播划分完成之后将结果保存下来，用于反向传播。

`python`代码实现如下：

```python
weight_grade = np.einsum('crlij,crl->c', self.split_input, input)
```

#### 对偏置项的梯度

对偏置项的梯度也是对每个通道求和即可。

```python
bias_grade = np.einsum('cij->c', input)
```

## 全连接层

### 设计思路

全连接层需要提供两个参数：

1. 输入大小
2. 输出大小

权重和偏置项均设置为$[-1,1)$的随机数。

### 前向传播

为了各层代码的一致性，我们在此也是用爱因斯坦求和约定计算。

`python`代码实现如下：

```python
self.output = np.einsum('i,ij->j',self.input,self.weight)\
            + self.bias
```

### 反向传播

首先进行`sigmoid`函数的反向传播。下面是用的输入均为传播之后的结果。

反向传播与实验一中的过程类似，在此处给出使用爱因斯坦求和约定的计算方式：

```python
input_grade = np.einsum('j,ij->i', input, self.weight)
weight_grade = np.einsum('i,j->ij', self.input, input)
bias_grade = np.sum(input, axis=0)
```

## 输出层

### 设计思路

输出层需要提供两个参数：

1. 输入大小
2. 输出大小

为了尽可能贴近原模型，使用PS绘制了大小为3点的0-9这十个数字的图像：

![1](./number.png)

在该大小下，恰好每个数字大小为12×7，与原模型中使用的大小一致。

我们定义`load_weight`方法，通过加载图像实现参数的确定：

```python
def load_weight(self, fp):
    im = Image.open(fp)
    arr = np.asarray(im)
    # 由于上一步输出的范围为0-1，所以这里将白色设置为0，黑色设置为1
    f = np.frompyfunc(lambda x:0 if x == 1 else 1, 1, 1)
    numbers = f(arr).astype(np.int32)
    split_number = np.split(numbers, 10, axis=1)
    for i in range(len(split_number)):
        split_number[i] = split_number[i].reshape((-1,))
    self.weight = np.array(split_number)
```

### 前向传播

前向传播基本是按照公式直接计算，但是有一点不同：由于按照RBF计算，越小越好，所以这里取相反数，使得结果越大越好.

```python
def forward(self, input):
    self.input = input
    self.output = np.zeros(self.output_size, dtype=np.float32)
    # 
    for i in range(self.output_size):
        for j in range(self.input_size):
            self.output[i] -= (input[j] - self.weight[i][j]) ** 2
    return self.output
```

### 反向传播

由于该层的参数由图像决定，所以不需要更新参数，只需要计算对输入的梯度即可。

```python
input_grade = np.zeros(self.input_size, dtype=np.float32)
for i in range(self.input_size):
    for j in range(self.output_size):
        input_grade[i] -= input[j] * 2 * (self.input[i] - self.weight[j][i])
return input_grade
```

## LeNet5搭建

### 基本思路

我们使用`LeNet5`类表示网络，采取小批量随机梯度下降的方法，所以在初始化网络时需要提供每次批量的数据个数。

在该类中，使用一个列表`networks`存储所有的层，使用一种较为直观的方式添加所有层：

```python
networks = []
def __init__(self, batch_size) -> None:
    self.batch_size = batch_size
    self.add(Conv((1,32,32),5,6))\
        .add(Subsampling((6,28,28),2))\
        .add(Conv((6,14,14),5,16))\
        .add(Subsampling((16,10,10),2))\
        .add(Conv((16,5,5),5,120))\
        .add(Connect(120,84))\
        .add(Output(84,10))

def add(self, net):
    self.networks.append(net)
    return self
```

### 训练过程



```python
def train(self, train_data: np.ndarray, y, iter=100, alpha=0.1):
    y_arg = np.argmax(y, axis=1)
    loss = []
    acc = []
    total_num = train_data.shape[0]
    for i in range(iter):
        curr_loss = 0
        for data, label in zip(train_data, y):
            if random.random() > self.batch_size / total_num:
                continue
            score = self.forward(data).T
            # print(score)
            # 计算softmax
            score -= np.max(score)
            exp_score = np.exp(score)
            p = exp_score / np.sum(exp_score)
            func = np.frompyfunc(lambda x:x if x != 0 else 1e-20,1,1)
            p = func(p).astype(np.float32).T
            # 计算交叉熵损失函数
            f = -np.sum(label*np.log(p))
            curr_loss += f
            # print(f)
            self.backprop(p-label , alpha)
        # 保存平均损失率
        loss.append(curr_loss / total_num)
        # 计算在训练集上的准确率
        preds = self.predict(train_data)
        curr_acc = 0
        for a, b in zip(preds, y_arg):
            if a == b:
                curr_acc += 1
        acc.append(curr_acc / total_num)
        print(f"epoch:{i+1}\tloss:{format(loss[-1], '.5f')}\tacc:{format(acc[-1], '.5f')}")
```

## 实验结果

